"""
    LSTM 单天多层预测，lgb,xgb等单天单层预测
    测试集进行单天预测以及结果评估：
        evaluate_model(),evaluate_forecasts()
    测试集进行滚动15天预测以及结果评估：
        forecast(),evaluate_model1(),RollingdataForEachLayer(),evaluate_forecasts1(),dataForEachLayer()
    最后提交结果：
        forecast(),evaluate_model1(),RollingdataForEachLayer()
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn.metrics as skm
from scipy.optimize import leastsq
import matplotlib
from sklearn.preprocessing import MinMaxScaler
import scipy.interpolate as spi
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.layers import RepeatVector, TimeDistributed
import math
from tensorflow.keras.models import load_model
from tensorflow import keras
import joblib
import os



# ---------------------------------------------------15次滚动预测--------------------------------------------------------------
# 滚动预测
def forecast(model, modelName,test_x):
    '''
    该函数实现对输入数据的预测
    '''
    # 每条数据
    test_x = np.array(test_x).reshape(1,-1)
    if (modelName == 'svr'):
        yhat = model.predict(test_x)
        yhat = yhat.reshape(yhat.shape[0], -1)
    if (modelName == 'rf'):
        yhat = model.predict(test_x)
        yhat = yhat.reshape(yhat.shape[0], -1)
    if (modelName == 'xgb'):
        yhat = model.predict(test_x)
        yhat = yhat.reshape(yhat.shape[0], -1)
    if (modelName == 'lgb'):
        yhat = model.predict(test_x)
        yhat = yhat.reshape(yhat.shape[0], -1)
    if (modelName == 'lstm'):
        input_x = test_x.reshape((1, 1, -1))
        yhat = model.predict(input_x, verbose=0)  # 预测下一天数据 [165,10]
    return yhat  # 数组 (10,)

# 滚动为输入数据加入变量
def RollingdataForEachLayer(x,min_max,level):
    """
    :param x: 数组x
    :param level: 土壤层数
    :return:
    """
    # series
    [x_max, x_min, y_max, y_min] = min_max
    x1 = x[0:24].reshape(1,-1)
    x_bd = x[24+level-1].reshape(1,-1)
    x_sand = x[34+level-1].reshape(1,-1)
    x_silt = x[44+level-1].reshape(1,-1)
    x_clay = x[54+level-1].reshape(1,-1)
    x_t_2 = x[64+level-1].reshape(1,-1)
    x_t_1 = x[74 + level - 1].reshape(1,-1)
    X = np.c_[x1, x_bd, x_sand, x_silt, x_clay, x_t_2, x_t_1]
    X[0,0:24] =(X[0,0:24] - x_min[0:24]) / (x_max[0:24] - x_min[0:24])
    X[0,28] = (X[0,28] - x_min[64+level-1]) / (x_max[64+level-1] - x_min[64+level-1])
    X[0,29] = (X[0,29] - x_min[74+level-1]) / (x_max[74+level-1] - x_min[74+level-1])
    Y_max = y_max[level-1]
    Y_min = y_min[level - 1]
    return X,[Y_max,Y_min]

# 评估滚动预测效果
def evaluate_model1(model,modelName, x,y,level):
    '''
        该函数实现模型评估
    '''
    # 读取最大最小值
    min_max = pd.read_csv('./data/min_max.csv',header=None)
    x_max= min_max[0]
    x_min = min_max[1]
    y_max = min_max[2]
    y_min = min_max[3]

    x=np.array(x)
    y=np.array(y)

    predictions = list()  # 用于保存每周的前向验证结果；
    start =0
    pre_length = 15
    # 往下滚动预测15次，计算误差
    for i in range(int(len(x)/pre_length)):
        start = i*pre_length
        end = start+pre_length
        history_fore = x[start:end, :]
        for j in range(pre_length):
            if (level >= 1 and level <= 10):
                test_x,[Y_max,Y_min] = RollingdataForEachLayer(history_fore[j,:],[x_max,x_min,y_max, y_min], level)
                yhat_sequence = forecast(model,modelName, test_x)  # 预测下周的数据
                predicts = yhat_sequence*(Y_max -Y_min)+Y_min
                predictions.append(predicts[0,0])  # 保存预测结果
                if (j < 14): history_fore[j + 1, 74+level-1] = predicts # 把预测结果添加到下一天的输入中
                if (j < 13): history_fore[j + 2, 64+level-1] = predicts
            else:
                test_x= (history_fore[j,:]-x_min)/(x_max-x_min)
                test_x[24:64]=history_fore[j,24:64]
                yhat_sequence = forecast(model, modelName, test_x)  # 预测下周的数据
                predicts = yhat_sequence[0,:]*(y_max[0:10] -y_min[0:10])+y_min[0:10]
                predictions.append(predicts)  # 保存预测结果
                if(j<14):history_fore[j+1,-10:]=predicts  # 把预测结果添加到下一天的输入中
                if(j<13):history_fore[j + 2, -20:-10] = predicts

    # 反归一化y值 [n,10]   评价预测结果
    if (level >=1):
        if os.path.exists('./data/test_predictions' + modelName + '.csv'):
            data = pd.read_csv('./data/test_predictions' + modelName + '.csv', header=None)
            data['new'] = np.array(predictions)
            pd.DataFrame(data).to_csv('./data/test_predictions' + modelName + '.csv', encoding='utf-8_sig',
                                      index=False, header=None)
        else:
            pd.DataFrame(np.array(predictions)).to_csv('./data/test_predictions' + modelName + '.csv',
                                                       encoding='utf-8_sig', index=False, header=None)
    else:
        pd.DataFrame(np.array(predictions)).to_csv('./data/test_predictions' + modelName + '.csv', encoding='utf-8_sig',
                                      index=False, header=None)

    return



# 评价真值和预测值
def evaluate_forecasts1(actual, predicted,pre_length):
    '''
        scores 每15天滚动预测 每个深度的的均方差
        R2 每15天滚动预测 每个深度的R2
        score 每天预测误差 的均方之和/样本/15天
    '''
    scores = list()
    R2 = []

    # [n,10] 分别计算每个深度的误差
    # 每15天样本的平均误差
    for m in range(int(actual.shape[0]/pre_length)):
        start = m * pre_length
        end = start + pre_length
        r2 = []
        rmse = []
        for i in range(actual.shape[1]):
            mse = skm.mean_squared_error(actual[start:end, i], predicted[start:end, i])
            rmse.append(math.sqrt(mse))
            r2.append(skm.r2_score(actual[start:end, i], predicted[start:end, i]))
        scores.append(np.array(rmse))
        R2.append(np.array(r2))

    s = 0
    score = [] # 每个样本10个深度总误差/样本数/深度，平均每个样本每个深度的误差
    for row in range(actual.shape[0]):
        for col in range(actual.shape[1]):
            s += (actual[row, col] - predicted[row, col]) ** 2
        if((row+1)/15==0):
            score.append(math.sqrt(s / (15 * actual.shape[1])))

    scores = np.array(scores)
    print('\n每15天滚动预测 每个深度的R2:',R2)
    print('\n每15天滚动预测 每个深度的RMSE:', scores)
    print('\n每天滚动预测 所有深度的RMSE:', score)
    np.savetxt('./evaluation/scores.csv',scores , delimiter=',')
    return score, scores

# ------------------------------------------------------------------------------------------------------------------------

# 评价真值和预测值
def evaluate_forecasts(actual, predicted):
    '''
        scores 每天 所有样本 的均方差
        score 每个样本 每天预测误差 的均方之和/样本，平均每个样本每个步长的误差
    '''
    scores = list()
    R2=[]
    # 所有样本，每层土壤湿度的预测rmse
    for i in range(actual.shape[1]):
        mse = skm.mean_squared_error(actual[:, i], predicted[:, i])
        rmse = math.sqrt(mse)
        scores.append(rmse)
        # 计算R2 多输出
        R2.append(skm.r2_score(actual[:,i], predicted[:,i]))


    s = 0  # 计算总的 RMSE
    # 平均每个样本，每层土壤湿度的预测误差
    for row in range(actual.shape[0]):
        for col in range(actual.shape[1]):
            s += (actual[row, col] - predicted[row, col]) ** 2
    score = math.sqrt(s / (actual.shape[0] * actual.shape[1]))
    print('actual.shape[0]:{}, actual.shape[1]:{}'.format(actual.shape[0], actual.shape[1]))

    print('\n 每个深度的R2:', R2)
    print('\n每个深度的RMSE:', scores)
    print('\n所有深度的RMSE:', score)
    scores = np.array(scores)
    # np.savetxt('./evaluation/scores.csv', scores, delimiter=',')

    # TODO 绘制真值和预测值的曲线
    plt.rcParams['font.family'] = 'Microsoft YaHei'
    plt.rcParams['axes.unicode_minus'] = False
    fig = plt.figure(figsize=(16, 10))
    plt.axes([0.1,0.7,0.8,0.2])
    plt.plot(actual[:,0], c='k',linestyle='-', linewidth=0.3,label='真值')
    plt.plot(predicted[:, 0], c='darkgray',linestyle='--', linewidth=0.3, label='预测值')
    plt.legend()
    plt.title('第1层'+'土壤水分含量', size=12)
    plt.axes([0.1, 0.4, 0.8, 0.2])
    plt.plot(actual[:, 4], c='k',linestyle='-', linewidth=0.3, label='真值')
    plt.plot(predicted[:, 4], c='darkgray',linestyle='--',  linewidth=0.3, label='预测值')
    plt.title('第5层' + '土壤水分含量', size=12)
    plt.axes([0.1, 0.1, 0.8, 0.2])
    plt.plot(actual[:, 8], c='k',linestyle='-', linewidth=0.3, label='真值')
    plt.plot(predicted[:, 8], c='darkgray',linestyle='--',  linewidth=0.3, label='预测值')
    plt.title('第9层' + '土壤水分含量', size=12)
    plt.savefig('Predictave.tif',dpi=300)

    fig = plt.figure( figsize=(8, 10))  # figsize=(10, 6), dpi=600
    for i in range(0, actual.shape[1],2): # actual.shape[1],
        plt.subplot(actual.shape[1], 1, i+1)
        feature_name = '第'+str(i+1)+'层'+'土壤水分含量'
        plt.plot(actual[:,i], c='orange', linewidth=0.1,label='真值')
        plt.plot(predicted[:,i], c='g',linewidth=0.1, label='预测值')
        plt.rcParams.update({'font.size': 4})
        plt.title(feature_name, size=5)  # y=0,
        plt.grid(linestyle='--', alpha=0.5)
        plt.xticks(fontsize=4)
        plt.yticks(fontsize=4)
        if(i==0):
            # plt.ylabel('水分含量(%)',fontsize=5)
            plt.legend()
        # fig.tight_layout()
        plt.subplots_adjust(left=0.1, bottom=0.1, right=None, top=None,wspace=None, hspace=0.1)

     # 调整整体空白
    fig.savefig('Predict and truth2.png',dpi=600)
    return score, scores

# 逐天预测
def evaluate_model(model,modelName, x,y,level):
    '''
        测试集预测和评估
    '''

    x=np.array(x)
    y=np.array(y)

    # 测试集单层预测
    if(level>=1 and level<=10):
        [test_x,test_y], scaler_testy=dataForEachLayer([x,y],level)
    else:
        # 多层预测（指 LSTM）
        [test_x, test_y], scaler_testy = scalerVariable(x, y,24,64)

    if(modelName == 'svr'):
        yhat = model.predict(test_x)
        yhat = yhat.reshape(yhat.shape[0],-1)
    if(modelName == 'rf'):
        yhat = model.predict(test_x)
        yhat = yhat.reshape(yhat.shape[0], -1)
    if (modelName == 'xgb'):
        yhat = model.predict(test_x)
        yhat = yhat.reshape(yhat.shape[0], -1)
    if (modelName == 'lgb'):
        yhat = model.predict(test_x)
        yhat = yhat.reshape(yhat.shape[0], -1)
    if(modelName == 'lstm'):
        input_x = test_x.reshape((test_x.shape[0], 1, -1))
        yhat = model.predict(input_x, verbose=0)  # 预测下一天数据 [165,10]

    # 反归一化y值 [n,10]   评价预测结果
    predicts = scaler_testy.inverse_transform(yhat)
    # 保存预测结果
    # pd.DataFrame(predicts).to_csv('./data/test_predictions'+str(level)+'.csv', encoding='utf-8_sig', index=False, header=None)
    #  mode='a'  追加行数据
    if(level>1):
        data = pd.read_csv('./data_addfeatures3/test_predictions' + modelName + '.csv',header=None)
        data['new'] = predicts
        pd.DataFrame(data).to_csv('./data_addfeatures3/test_predictions' + modelName + '.csv', encoding='utf-8_sig',
                                      index=False, header=None)
    else: pd.DataFrame(predicts).to_csv('./data_addfeatures3/test_predictions' + modelName + '.csv', encoding='utf-8_sig',
                                  index=False, header=None)
    # 测试集精度评估
    score, scores = evaluate_forecasts(scaler_testy.inverse_transform(test_y), predicts)
    print()
    return score, scores


# 归一化
def scalerVariable(x,y,m,n):
    scaler_x = MinMaxScaler(feature_range=(0, 1))
    X = scaler_x.fit_transform(x)
    scaler_y = MinMaxScaler(feature_range=(0, 1))
    Y = scaler_y.fit_transform(y.reshape(x.shape[0],-1))
    # 注意为常量的bulk density和土壤组成 不进行缩放  24：63列
    # X[:, m:n] = x[:, m:n]
    return [X,Y],scaler_y

# 划分每层数据
def dataForEachLayer(data,level):
    """
    :param x: 数组x
    :param y: 数组y
    :param level: 土壤层数
    :return:
    """
    [x,y] =data
    x1 = x[:,0:24]
    x_bd = x[:,24+level-1]
    x_sand = x[:,34+level-1]
    x_silt = x[:,44+level-1]
    x_clay = x[:,54+level-1]
    x_t_2 = x[:,64+level-1]
    x_t_1 = x[:, 74 + level - 1]
    X=np.c_[x1,x_bd,x_sand,x_silt,x_clay,x_t_2,x_t_1]
    Y=y[:,level-1]
    # 先划分再归一化
    data, scaler_y = scalerVariable(X, Y, 24, 28)

    return data,scaler_y


sliding_window_width = 1
input_sequence_start = 0
# 测试集预测 
test_x= pd.read_csv("./data/test_result/集成/test_x.csv", header=None)
test_y = pd.read_csv("./data/test_result/集成/test_y.csv", header=None)
modelName='lgb'

# TODO 由于test数据6个预测点时间不连续，所以测试时，需要分开在6个测试集测试
# lstm预
# model = load_model('./save_model/lstm_200_200_200_100_relu_all/LSTM.723-0.0011-0.9045-best.h5')
# svr预测
# model = joblib.load('./save_model/svr_model/svr_level10.pkl')
# rf预测
# model1 = joblib.load('./save_model/rf_model_all/rf_level1.pkl')
# model2 = joblib.load('./save_model/rf_model_all/rf_level2.pkl')
# model3 = joblib.load('./save_model/rf_model_all/rf_level3.pkl')
# model4 = joblib.load('./save_model/rf_model_all/rf_level4.pkl')
# model5 = joblib.load('./save_model/rf_model_all/rf_level5.pkl')
# model6 = joblib.load('./save_model/rf_model_all/rf_level6.pkl')
# model7 = joblib.load('./save_model/rf_model_all/rf_level7.pkl')
# model8 = joblib.load('./save_model/rf_model_all/rf_level8.pkl')
# model9 = joblib.load('./save_model/rf_model_all/rf_level9.pkl')
# model10 = joblib.load('./save_model/rf_model_all/rf_level10.pkl')

# lightgbm预测
# model1 = joblib.load('./save_model3/lgb_model/lgb_level1.pkl')
# model2 = joblib.load('./save_model/lgb_model/lgb_level2.pkl')
# model3 = joblib.load('./save_model/lgb_model/lgb_level3.pkl')
# model4 = joblib.load('./save_model/lgb_model_all/lgb_level4.pkl')
# model5 = joblib.load('./save_model/lgb_model_all/lgb_level5.pkl')
# model6 = joblib.load('./save_model/lgb_model_all/lgb_level6.pkl')
# model7 = joblib.load('./save_model/lgb_model_all/lgb_level7.pkl')
# model8 = joblib.load('./save_model/lgb_model_all/lgb_level8.pkl')
# model9 = joblib.load('./save_model/lgb_model_all/lgb_level9.pkl')
# model10 = joblib.load('./save_model/lgb_model_all/lgb_level10.pkl')

# xgboost 预测
# model1 = joblib.load('./save_model/xgb_model_all/xgb_level1.pkl')
# model2 = joblib.load('./save_model/xgb_model_all/xgb_level2.pkl')
# model3 = joblib.load('./save_model/xgb_model_all/xgb_level3.pkl')
# model4 = joblib.load('./save_model/xgb_model_all/xgb_level4.pkl')
# model5 = joblib.load('./save_model/xgb_model_all/xgb_level5.pkl')
# model6 = joblib.load('./save_model/xgb_model_all/xgb_level6.pkl')
# model7 = joblib.load('./save_model/xgb_model_all/xgb_level7.pkl')
# model8 = joblib.load('./save_model/xgb_model_all/xgb_level8.pkl')
# model9 = joblib.load('./save_model/xgb_model_all/xgb_level9.pkl')
# model10 = joblib.load('./save_model/xgb_model_all/xgb_level10.pkl')

# 单天预测
evaluations = evaluate_model(model,modelName, test_x, test_y,level=1)


#---------------------------------------------------模型集成精度评估--------------------------------------------------------------

# 计算模型均值，评价预测效果
predict_lstm = pd.read_csv('./data/test_result/集成/test_predictionslstm.csv',header=None)
predict_rf = pd.read_csv('./data/test_result/集成/test_predictionsrf.csv',header=None)
predict_xgb = pd.read_csv('./data/test_result/集成/test_predictionsxgb.csv',header=None)
predict_lgb  = pd.read_csv('./data/test_result/集成/test_predictionslgb.csv',header=None)
# 加权结果
# weight =np.array([[0.48	,0.32,	0.22,	0.22,	0.2	,	0.19,	0.22,	0.07,	0.07,	0.06],
# [0.18,	0.24,0.28,	0.25,	0.29,	0.29,	0.27,	0.31,	0.32,	0.31],
# [0.18,	0.23,	0.26,	0.29,	0.26,	0.28,	0.27,	0.32,	0.31,	0.32],
# [0.16,	0.21,	0.24,	0.24,	0.25,	0.24,	0.24,	0.3,0.3,0.31]])
predict_ave=pd.DataFrame()  # n*10
# for i in range(10):
#     predict_ave[i] = weight[0,i]*np.array(predict_lstm[i])+weight[3,i]*np.array(predict_rf[i])+weight[2,i]*np.array(predict_xgb[i])+weight[1,i]*np.array(predict_xgb[i])
# 取均值
predict_ave = (np.array(predict_lstm)+np.array(predict_xgb)+np.array(predict_rf)+np.array(predict_lgb))/4
# for i in [4,5,8]:
#     predict_ave[i] = (np.array(predict_xgb[i]) + np.array(predict_rf[i]) + np.array(predict_lgb[i])) / 3
# for j in [2,3,6,7]:
#     predict_ave[j] = (np.array(predict_lstm[j]) + np.array(predict_xgb[j]) + np.array(predict_lgb[j])) / 3
# for m in [0,1,9]:
#     predict_ave[m] = (np.array(predict_lstm[m]) + np.array(predict_xgb[m]) + np.array(predict_rf[m]) + np.array(predict_lgb[m])) / 4
# predict_ave = predict_ave[[0,1,2,3,4,5,6,7,8,9]]
# pd.DataFrame(predict_ave).to_csv('./data/test_result/集成/predict_ave_selectmodel.csv',encoding='utf-8_sig', index=False, header=None)

evaluate_forecasts(np.array(test_y),np.array(predict_ave))

# 滚动15天预测
# evaluations = evaluate_model(model1, modelName, test_x, test_y,level=1)
# evaluations = evaluate_model1(model2, modelName, test_x, test_y,level=2)
# evaluations = evaluate_model1(model3, modelName, test_x, test_y,level=3)
# evaluations = evaluate_model1(model4, modelName, test_x, test_y,level=4)
# evaluations = evaluate_model1(model5, modelName, test_x, test_y,level=5)
# evaluations = evaluate_model1(model6, modelName, test_x, test_y,level=6)
# evaluations = evaluate_model1(model7, modelName, test_x, test_y,level=7)
# evaluations = evaluate_model1(model8, modelName, test_x, test_y,level=8)
# evaluations = evaluate_model1(model9, modelName, test_x, test_y,level=9)
# evaluations = evaluate_model1(model10, modelName, test_x, test_y,level=10)
print('end!')


# 绘制lstm训练精度曲线
# data = pd.read_excel('模型参数.xlsx',sheet_name='Sheet5',header=None)
# epochs = np.arange(1,1001,1)
# # acc=data.iloc[:,0]
# acc=[0.11929203569889069, 0.3692035377025604, 0.4000000059604645, 0.36424779891967773, 0.38017699122428894, 0.39752212166786194, 0.42159292101860046, 0.4184070825576782, 0.476106196641922, 0.5253097414970398, 0.5614159107208252, 0.5879645943641663, 0.5932743549346924, 0.5968141555786133, 0.5964601635932922, 0.6226548552513123, 0.6208849549293518, 0.6205309629440308, 0.6283186078071594, 0.6339823007583618, 0.6329203248023987, 0.6290265321731567, 0.6403539776802063, 0.6431858539581299, 0.6502655148506165, 0.6576991081237793, 0.6615929007530212, 0.6672566533088684, 0.6605309844017029, 0.6743362545967102, 0.6902654767036438, 0.6824778914451599, 0.6895574927330017, 0.7040708065032959, 0.6994690299034119, 0.7012389302253723, 0.7076106071472168, 0.7196460366249084, 0.709026575088501, 0.722123920917511, 0.743716835975647, 0.7380530834197998, 0.7373451590538025, 0.739469051361084, 0.7423008680343628, 0.7557522058486938, 0.7553982138633728, 0.7585840821266174, 0.7536283135414124, 0.7543362975120544, 0.7543362975120544, 0.752566397190094, 0.7391150593757629, 0.7497345209121704, 0.7688495516777039, 0.7642477750778198, 0.7624778747558594, 0.7635398507118225, 0.7582300901412964, 0.7635398507118225, 0.7720354199409485, 0.7561061978340149, 0.7713274359703064, 0.7511504292488098, 0.7603539824485779, 0.761415958404541, 0.7692035436630249, 0.7674336433410645, 0.7564601898193359, 0.7702654600143433, 0.769911527633667, 0.769911527633667, 0.7727433443069458, 0.7738053202629089, 0.7550442218780518, 0.7815929055213928, 0.7713274359703064, 0.7638937830924988, 0.765309751033783, 0.7578760981559753, 0.7780531048774719, 0.78725665807724, 0.7844247817993164, 0.782654881477356, 0.7769911289215088, 0.783008873462677, 0.791504442691803, 0.7858406901359558, 0.7738053202629089, 0.7890265583992004, 0.7840707898139954, 0.7780531048774719, 0.7876105904579163, 0.78725665807724, 0.7907964587211609, 0.800000011920929, 0.7982301115989685, 0.8021239042282104, 0.7950442433357239, 0.795752227306366, 0.7968141436576843, 0.7975221276283264, 0.8010619282722473, 0.7911504507064819, 0.809203565120697, 0.7890265583992004, 0.8113274574279785, 0.8070796728134155, 0.8067256808280945, 0.8017699122428894, 0.8155752420425415, 0.8081415891647339, 0.817345142364502, 0.8148672580718994, 0.804601788520813, 0.8106194734573364, 0.8007079362869263, 0.791858434677124, 0.8159291744232178, 0.8194690346717834, 0.8024778962135315, 0.795752227306366, 0.8123893737792969, 0.8208849430084229, 0.8035398125648499, 0.8198230266571045, 0.8084955811500549, 0.8191150426864624, 0.826194703578949, 0.8063716888427734, 0.818053126335144, 0.8084955811500549, 0.8074336051940918, 0.8269026279449463, 0.813097357749939, 0.8148672580718994, 0.8070796728134155, 0.817345142364502, 0.8095574975013733, 0.8212389349937439, 0.8258407115936279, 0.8283185958862305, 0.821946918964386, 0.8226548433303833, 0.8244248032569885, 0.8166371583938599, 0.8077875971794128, 0.8247787356376648, 0.8258407115936279, 0.8201770186424255, 0.8113274574279785, 0.8102654814720154, 0.8191150426864624, 0.8113274574279785, 0.8258407115936279, 0.82654869556427, 0.81345134973526, 0.8187610507011414, 0.8276106119155884, 0.8067256808280945, 0.8162831664085388, 0.8297345042228699, 0.8187610507011414, 0.8148672580718994, 0.8191150426864624, 0.817345142364502, 0.813097357749939, 0.8141592741012573, 0.8325663805007935, 0.822300910949707, 0.8230088353157043, 0.8162831664085388, 0.8226548433303833, 0.8205309510231018, 0.8226548433303833, 0.8293805122375488, 0.813097357749939, 0.8226548433303833, 0.8237168192863464, 0.8269026279449463, 0.8247787356376648, 0.8191150426864624, 0.8357521891593933, 0.8329203724861145, 0.8258407115936279, 0.8230088353157043, 0.826194703578949, 0.8212389349937439, 0.8286725878715515, 0.830796480178833, 0.8315044045448303, 0.8290265202522278, 0.8162831664085388, 0.818053126335144, 0.8329203724861145, 0.8318583965301514, 0.831150472164154, 0.8127433657646179, 0.8364601731300354, 0.8169911503791809, 0.8244248032569885, 0.8269026279449463, 0.8251327276229858, 0.831150472164154, 0.8169911503791809, 0.831150472164154, 0.8315044045448303, 0.8127433657646179, 0.8258407115936279, 0.826194703578949, 0.82654869556427, 0.8191150426864624, 0.835398256778717, 0.8325663805007935, 0.8361061811447144, 0.8215929269790649, 0.8226548433303833, 0.830796480178833, 0.82654869556427, 0.8283185958862305, 0.8194690346717834, 0.8283185958862305, 0.8300884962081909, 0.822300910949707, 0.822300910949707, 0.8272566199302673, 0.8361061811447144, 0.8162831664085388, 0.8378760814666748, 0.8300884962081909, 0.8276106119155884, 0.8318583965301514, 0.826194703578949, 0.8336282968521118, 0.835044264793396, 0.835044264793396, 0.8329203724861145, 0.8233628273010254, 0.8343362808227539, 0.8155752420425415, 0.817699134349823, 0.82654869556427, 0.8169911503791809, 0.8279646039009094, 0.8279646039009094, 0.8325663805007935, 0.831150472164154, 0.8364601731300354, 0.831150472164154, 0.8399999737739563, 0.8382300734519958, 0.8279646039009094, 0.835398256778717, 0.8329203724861145, 0.8297345042228699, 0.8230088353157043, 0.8371681571006775, 0.8283185958862305, 0.831150472164154, 0.8293805122375488, 0.8290265202522278, 0.8215929269790649, 0.8251327276229858, 0.8364601731300354, 0.8424778580665588, 0.8361061811447144, 0.8460177183151245, 0.8258407115936279, 0.8325663805007935, 0.835398256778717, 0.835398256778717, 0.8407079577445984, 0.826194703578949, 0.8290265202522278, 0.8428318500518799, 0.8428318500518799, 0.8357521891593933, 0.8385840654373169, 0.8368141651153564, 0.8449557423591614, 0.8385840654373169, 0.8336282968521118, 0.8399999737739563, 0.843539834022522, 0.8449557423591614, 0.8463717103004456, 0.8403539657592773, 0.843539834022522, 0.8449557423591614, 0.83964604139328, 0.8378760814666748, 0.8414159417152405, 0.835398256778717, 0.8368141651153564, 0.8247787356376648, 0.8467256426811218, 0.8474336266517639, 0.8474336266517639, 0.8414159417152405, 0.8389380574226379, 0.8449557423591614, 0.8428318500518799, 0.8375221490859985, 0.8339822888374329, 0.8237168192863464, 0.8428318500518799, 0.8339822888374329, 0.8300884962081909, 0.8399999737739563, 0.848141610622406, 0.8403539657592773, 0.8552212119102478, 0.8463717103004456, 0.8467256426811218, 0.8385840654373169, 0.8254867196083069, 0.83964604139328, 0.8446017503738403, 0.843539834022522, 0.835044264793396, 0.85274338722229, 0.839292049407959, 0.8389380574226379, 0.8424778580665588, 0.8410619497299194, 0.852389395236969, 0.847787618637085, 0.8375221490859985, 0.843539834022522, 0.8538053035736084, 0.8474336266517639, 0.8293805122375488, 0.8339822888374329, 0.8573451042175293, 0.843893826007843, 0.843539834022522, 0.843893826007843, 0.8502655029296875, 0.83964604139328, 0.83964604139328, 0.8361061811447144, 0.8509734272956848, 0.8414159417152405, 0.835398256778717, 0.852389395236969, 0.8407079577445984, 0.8403539657592773, 0.8513274192810059, 0.8428318500518799, 0.8431858420372009, 0.848141610622406, 0.8417699337005615, 0.8424778580665588, 0.8237168192863464, 0.848495602607727, 0.8453097343444824, 0.8421238660812378, 0.8502655029296875, 0.8407079577445984, 0.8513274192810059, 0.8467256426811218, 0.8378760814666748, 0.8361061811447144, 0.848495602607727, 0.8329203724861145, 0.8403539657592773, 0.8399999737739563, 0.8584070801734924, 0.8470796346664429, 0.8559291958808899, 0.852389395236969, 0.8502655029296875, 0.8407079577445984, 0.8456637263298035, 0.835398256778717, 0.8421238660812378, 0.8470796346664429, 0.8410619497299194, 0.8322123885154724, 0.8559291958808899, 0.8492035269737244, 0.8453097343444824, 0.8605309724807739, 0.8573451042175293, 0.8538053035736084, 0.8456637263298035, 0.8492035269737244, 0.8559291958808899, 0.8598229885101318, 0.852389395236969, 0.8456637263298035, 0.8534513115882874, 0.8414159417152405, 0.8463717103004456, 0.8530973196029663, 0.8587610721588135, 0.8431858420372009, 0.852389395236969, 0.8580530881881714, 0.8552212119102478, 0.8587610721588135, 0.8428318500518799, 0.8428318500518799, 0.847787618637085, 0.8509734272956848, 0.8467256426811218, 0.8509734272956848, 0.8584070801734924, 0.8456637263298035, 0.8545132875442505, 0.8534513115882874, 0.852035403251648, 0.865486741065979, 0.8538053035736084, 0.8711504340171814, 0.8506194949150085, 0.8424778580665588, 0.8605309724807739, 0.8495575189590454, 0.8538053035736084, 0.8584070801734924, 0.8538053035736084, 0.8513274192810059, 0.8591150641441345, 0.8562831878662109, 0.8591150641441345, 0.8637168407440186, 0.8559291958808899, 0.8573451042175293, 0.8584070801734924, 0.8576990962028503, 0.8538053035736084, 0.8456637263298035, 0.8453097343444824, 0.8364601731300354, 0.8453097343444824, 0.847787618637085, 0.8492035269737244, 0.8559291958808899, 0.8513274192810059, 0.8587610721588135, 0.8626548647880554, 0.8658407330513, 0.865486741065979, 0.8534513115882874, 0.8637168407440186, 0.8594690561294556, 0.870088517665863, 0.8619468808174133, 0.8594690561294556, 0.856637179851532, 0.865486741065979, 0.865132749080658, 0.8623008728027344, 0.8753982186317444, 0.8658407330513, 0.8587610721588135, 0.870088517665863, 0.8669026494026184, 0.8555752038955688, 0.8573451042175293, 0.8562831878662109, 0.873982310295105, 0.8637168407440186, 0.8672566413879395, 0.85274338722229, 0.856637179851532, 0.861238956451416, 0.8665486574172974, 0.8686725497245789, 0.873982310295105, 0.865486741065979, 0.8633628487586975, 0.8559291958808899, 0.8799999952316284, 0.852035403251648, 0.8637168407440186, 0.8368141651153564, 0.8545132875442505, 0.8619468808174133, 0.8619468808174133, 0.8729203343391418, 0.8587610721588135, 0.8637168407440186, 0.8644247651100159, 0.8707964420318604, 0.8729203343391418, 0.878584086894989, 0.8690265417098999, 0.8630088567733765, 0.8619468808174133, 0.8736283183097839, 0.8711504340171814, 0.8499115109443665, 0.8729203343391418, 0.8707964420318604, 0.873982310295105, 0.8605309724807739, 0.8669026494026184, 0.8690265417098999, 0.8746902942657471, 0.83964604139328, 0.8718584179878235, 0.8669026494026184, 0.8679646253585815, 0.8669026494026184, 0.8746902942657471, 0.8722124099731445, 0.8633628487586975, 0.865486741065979, 0.8736283183097839, 0.869380533695221, 0.8676106333732605, 0.8669026494026184, 0.8792920112609863, 0.8729203343391418, 0.8686725497245789, 0.8661946654319763, 0.8587610721588135, 0.8704424500465393, 0.870088517665863, 0.8658407330513, 0.8686725497245789, 0.8799999952316284, 0.8591150641441345, 0.8357521891593933, 0.8290265202522278, 0.8463717103004456, 0.847787618637085, 0.8555752038955688, 0.8732743263244629, 0.8640707731246948, 0.8676106333732605, 0.8683185577392578, 0.8838937878608704, 0.8732743263244629, 0.8771681189537048, 0.8661946654319763, 0.8676106333732605, 0.8768141865730286, 0.8633628487586975, 0.8775221109390259, 0.8686725497245789, 0.870088517665863, 0.8722124099731445, 0.8686725497245789, 0.869734525680542, 0.8647787570953369, 0.8718584179878235, 0.869380533695221, 0.8761062026023865, 0.869380533695221, 0.8750442266464233, 0.8722124099731445, 0.8757522106170654, 0.869734525680542, 0.8690265417098999, 0.8707964420318604, 0.8637168407440186, 0.8538053035736084, 0.8792920112609863, 0.8725663423538208, 0.8707964420318604, 0.8661946654319763, 0.865132749080658, 0.878230094909668, 0.8753982186317444, 0.8757522106170654, 0.8792920112609863, 0.860884964466095, 0.870088517665863, 0.874336302280426, 0.8761062026023865, 0.8732743263244629, 0.878230094909668, 0.8761062026023865, 0.8658407330513, 0.8757522106170654, 0.8750442266464233, 0.8690265417098999, 0.8669026494026184, 0.8746902942657471, 0.8814159035682678, 0.874336302280426, 0.848495602607727, 0.8601769804954529, 0.8736283183097839, 0.8789380788803101, 0.869380533695221, 0.8725663423538208, 0.8810619711875916, 0.878230094909668, 0.8789380788803101, 0.8690265417098999, 0.8768141865730286, 0.8789380788803101, 0.869734525680542, 0.8644247651100159, 0.8860176801681519, 0.8753982186317444, 0.8640707731246948, 0.8821238875389099, 0.8853097558021545, 0.8761062026023865, 0.8849557638168335, 0.8835397958755493, 0.8764601945877075, 0.8750442266464233, 0.8676106333732605, 0.8764601945877075, 0.870088517665863, 0.8803539872169495, 0.8704424500465393, 0.8672566413879395, 0.8753982186317444, 0.8715044260025024, 0.8775221109390259, 0.8683185577392578, 0.878230094909668, 0.8704424500465393, 0.8821238875389099, 0.8863716721534729, 0.8860176801681519, 0.8715044260025024, 0.8761062026023865, 0.8835397958755493, 0.8796460032463074, 0.8799999952316284, 0.8817698955535889, 0.8888495564460754, 0.887079656124115, 0.8807079792022705, 0.887079656124115, 0.8853097558021545, 0.8849557638168335, 0.8817698955535889, 0.8732743263244629, 0.8884955644607544, 0.8761062026023865, 0.8683185577392578, 0.8587610721588135, 0.882831871509552, 0.8846017718315125, 0.8605309724807739, 0.8796460032463074, 0.887079656124115, 0.891681432723999, 0.883185863494873, 0.8838937878608704, 0.8853097558021545, 0.8920354247093201, 0.8796460032463074, 0.8920354247093201, 0.8807079792022705, 0.8906194567680359, 0.8962832093238831, 0.883185863494873, 0.8729203343391418, 0.8778761029243469, 0.8888495564460754, 0.8835397958755493, 0.8920354247093201, 0.8810619711875916, 0.8899115324020386, 0.8810619711875916, 0.8502655029296875, 0.8640707731246948, 0.8750442266464233, 0.8838937878608704, 0.8923893570899963, 0.887433648109436, 0.891681432723999, 0.9019469022750854, 0.9015929102897644, 0.886725664138794, 0.8768141865730286, 0.8923893570899963, 0.8835397958755493, 0.8860176801681519, 0.8920354247093201, 0.895575225353241, 0.8849557638168335, 0.8892035484313965, 0.8906194567680359, 0.8906194567680359, 0.882477879524231, 0.8853097558021545, 0.8633628487586975, 0.887079656124115, 0.886725664138794, 0.8984071016311646, 0.8877876400947571, 0.8895575404167175, 0.895575225353241, 0.8757522106170654, 0.882477879524231, 0.8803539872169495, 0.8810619711875916, 0.8923893570899963, 0.8630088567733765, 0.8637168407440186, 0.8902654647827148, 0.8764601945877075, 0.8842477798461914, 0.8930973410606384, 0.8906194567680359, 0.8835397958755493, 0.8973451256752014, 0.8849557638168335, 0.8941593170166016, 0.895575225353241, 0.891327440738678, 0.9040707945823669, 0.8920354247093201, 0.8810619711875916, 0.8881415724754333, 0.8973451256752014, 0.8962832093238831, 0.8980531096458435, 0.895929217338562, 0.873982310295105, 0.878584086894989, 0.8881415724754333, 0.8976991176605225, 0.8994690179824829, 0.9040707945823669, 0.9033628106117249, 0.8962832093238831, 0.8952212333679199, 0.8927433490753174, 0.8778761029243469, 0.8722124099731445, 0.8909734487533569, 0.8980531096458435, 0.8966371417045593, 0.9008849263191223, 0.9030088782310486, 0.8973451256752014, 0.900177001953125, 0.9015929102897644, 0.8969911336898804, 0.9104424715042114, 0.8729203343391418, 0.8658407330513, 0.8838937878608704, 0.8269026279449463, 0.8757522106170654, 0.8938053250312805, 0.9019469022750854, 0.9072566628456116, 0.904424786567688, 0.9061946868896484, 0.909026563167572, 0.9069026708602905, 0.8991150259971619, 0.9015929102897644, 0.9079645872116089, 0.9012389183044434, 0.904424786567688, 0.9037168025970459, 0.9072566628456116, 0.9100884795188904, 0.899823009967804, 0.904424786567688, 0.8984071016311646, 0.9008849263191223, 0.9012389183044434, 0.9012389183044434, 0.9037168025970459, 0.8976991176605225, 0.8984071016311646, 0.8909734487533569, 0.891681432723999, 0.8976991176605225, 0.8991150259971619, 0.886725664138794, 0.913274347782135, 0.9143362641334534, 0.9104424715042114, 0.908672571182251, 0.9069026708602905, 0.8976991176605225, 0.9122123718261719, 0.9054867029190063, 0.9033628106117249, 0.9058406949043274, 0.9065486788749695, 0.8778761029243469, 0.7975221276283264, 0.8488495349884033, 0.847787618637085, 0.8633628487586975, 0.8941593170166016, 0.8927433490753174, 0.8920354247093201, 0.9015929102897644, 0.9026548862457275, 0.9104424715042114, 0.9065486788749695, 0.9122123718261719, 0.9118583798408508, 0.909026563167572, 0.908672571182251, 0.9115044474601746, 0.9203540086746216, 0.917522132396698, 0.9153982400894165, 0.913274347782135, 0.9168141484260559, 0.909026563167572, 0.9118583798408508, 0.9153982400894165, 0.917522132396698, 0.9157522320747375, 0.9161062240600586, 0.9196460247039795, 0.904424786567688, 0.9051327705383301, 0.9054867029190063, 0.9015929102897644, 0.8580530881881714, 0.8718584179878235, 0.9100884795188904, 0.909026563167572, 0.9189380407333374, 0.9093805551528931, 0.9146902561187744, 0.9139822721481323, 0.9146902561187744, 0.9051327705383301, 0.917876124382019, 0.9164601564407349, 0.913274347782135, 0.9168141484260559, 0.9153982400894165, 0.9093805551528931, 0.9012389183044434, 0.8980531096458435, 0.8807079792022705, 0.8987610340118408, 0.9100884795188904, 0.9122123718261719, 0.912920355796814, 0.9111504554748535, 0.913274347782135, 0.9139822721481323, 0.917168140411377, 0.9111504554748535, 0.9164601564407349, 0.913628339767456, 0.9224779009819031, 0.9192920327186584, 0.9153982400894165, 0.9104424715042114, 0.909026563167572, 0.9256637096405029, 0.9115044474601746, 0.8718584179878235, 0.8584070801734924, 0.8729203343391418, 0.8361061811447144, 0.8509734272956848, 0.886725664138794, 0.886725664138794, 0.9008849263191223, 0.9118583798408508, 0.9150442481040955, 0.9122123718261719, 0.9161062240600586, 0.9157522320747375, 0.9143362641334534, 0.9153982400894165, 0.9228318333625793, 0.9182301163673401, 0.9224779009819031, 0.917168140411377, 0.9111504554748535, 0.9200000166893005, 0.9207079410552979, 0.9157522320747375, 0.917168140411377, 0.9104424715042114, 0.9253097176551819, 0.9093805551528931, 0.922123908996582, 0.9168141484260559, 0.9069026708602905, 0.9200000166893005, 0.9153982400894165, 0.9192920327186584, 0.9125663638114929, 0.9210619330406189, 0.9104424715042114, 0.9185840487480164, 0.9150442481040955, 0.909026563167572, 0.9153982400894165, 0.9143362641334534, 0.9065486788749695, 0.8877876400947571, 0.895575225353241, 0.8948672413825989, 0.9023008942604065, 0.8881415724754333, 0.8948672413825989, 0.9097344875335693, 0.9157522320747375, 0.9157522320747375, 0.922123908996582, 0.9185840487480164, 0.9069026708602905, 0.9069026708602905, 0.8895575404167175, 0.9125663638114929, 0.913274347782135, 0.9026548862457275, 0.913274347782135, 0.917168140411377, 0.9143362641334534, 0.9231858253479004, 0.9235398173332214, 0.917522132396698, 0.9196460247039795, 0.9164601564407349, 0.9164601564407349, 0.913628339767456, 0.9161062240600586, 0.9182301163673401, 0.9115044474601746, 0.917168140411377, 0.904424786567688, 0.9058406949043274, 0.908672571182251, 0.9189380407333374, 0.9200000166893005, 0.900530993938446, 0.856991171836853, 0.8803539872169495, 0.9107964634895325, 0.917522132396698, 0.9146902561187744, 0.9235398173332214, 0.917522132396698, 0.9153982400894165, 0.9153982400894165, 0.922123908996582, 0.9192920327186584, 0.9277876019477844, 0.913628339767456, 0.9143362641334534, 0.9207079410552979, 0.9189380407333374, 0.9182301163673401, 0.9122123718261719, 0.912920355796814, 0.8962832093238831, 0.8934513330459595, 0.8991150259971619, 0.9097344875335693, 0.8976991176605225, 0.895929217338562, 0.917522132396698, 0.9153982400894165, 0.9125663638114929, 0.9207079410552979, 0.9200000166893005, 0.9224779009819031, 0.9235398173332214, 0.9164601564407349, 0.9238938093185425, 0.9139822721481323, 0.9150442481040955, 0.9104424715042114, 0.9107964634895325, 0.9192920327186584, 0.9146902561187744, 0.8846017718315125, 0.860884964466095, 0.9040707945823669, 0.9185840487480164, 0.85274338722229, 0.8888495564460754, 0.9079645872116089, 0.891327440738678, 0.904778778553009, 0.9139822721481323, 0.9228318333625793, 0.9196460247039795, 0.9189380407333374, 0.9200000166893005, 0.9214159250259399, 0.9231858253479004, 0.9277876019477844, 0.9235398173332214, 0.9189380407333374, 0.9214159250259399, 0.9182301163673401, 0.9235398173332214, 0.917168140411377, 0.9235398173332214]
# # val_acc = data.iloc[:,1]
# val_acc = [0.23885349929332733, 0.35031846165657043, 0.337579607963562, 0.36624205112457275, 0.3949044644832611, 0.41719746589660645, 0.36305731534957886, 0.5095541477203369, 0.5668789744377136, 0.6146496534347534, 0.6242038011550903, 0.5955414175987244, 0.5350318551063538, 0.6433120965957642, 0.6433120965957642, 0.6464968323707581, 0.6337579488754272, 0.6656050682067871, 0.6019108295440674, 0.6082802414894104, 0.6401273608207703, 0.6242038011550903, 0.6337579488754272, 0.675159215927124, 0.6433120965957642, 0.5859872698783875, 0.6942675113677979, 0.7388535141944885, 0.6974522471427917, 0.7261146306991577, 0.7070063948631287, 0.6496815085411072, 0.7261146306991577, 0.7452229261398315, 0.7070063948631287, 0.6592356562614441, 0.7738853693008423, 0.7643312215805054, 0.7579618096351624, 0.7579618096351624, 0.7707006335258484, 0.7707006335258484, 0.7484076619148254, 0.7197452187538147, 0.8121019005775452, 0.7643312215805054, 0.7356687784194946, 0.7770700454711914, 0.7579618096351624, 0.7070063948631287, 0.7675158977508545, 0.7515923380851746, 0.7770700454711914, 0.7006369233131409, 0.7866241931915283, 0.7898089289665222, 0.7834395170211792, 0.7929936051368713, 0.7611464858055115, 0.7643312215805054, 0.7547770738601685, 0.7898089289665222, 0.7866241931915283, 0.7802547812461853, 0.6878980994224548, 0.7388535141944885, 0.824840784072876, 0.7770700454711914, 0.7547770738601685, 0.7579618096351624, 0.8152866363525391, 0.8121019005775452, 0.7738853693008423, 0.808917224407196, 0.7961783409118652, 0.7770700454711914, 0.7770700454711914, 0.7547770738601685, 0.7961783409118652, 0.7993630766868591, 0.8184713125228882, 0.7643312215805054, 0.8216560482978821, 0.8184713125228882, 0.7834395170211792, 0.8121019005775452, 0.824840784072876, 0.7834395170211792, 0.7770700454711914, 0.8407643437385559, 0.8503184914588928, 0.7611464858055115, 0.7707006335258484, 0.7707006335258484, 0.824840784072876, 0.8057324886322021, 0.824840784072876, 0.8407643437385559, 0.7802547812461853, 0.8471337556838989, 0.837579607963562, 0.8152866363525391, 0.808917224407196, 0.8025477528572083, 0.7452229261398315, 0.7898089289665222, 0.831210196018219, 0.831210196018219, 0.8662420511245728, 0.7961783409118652, 0.8216560482978821, 0.8630573153495789, 0.831210196018219, 0.8535031676292419, 0.824840784072876, 0.8598726391792297, 0.824840784072876, 0.831210196018219, 0.8503184914588928, 0.8535031676292419, 0.7898089289665222, 0.7579618096351624, 0.8407643437385559, 0.8121019005775452, 0.8343949317932129, 0.8184713125228882, 0.831210196018219, 0.8407643437385559, 0.7929936051368713, 0.8535031676292419, 0.8662420511245728, 0.8152866363525391, 0.8630573153495789, 0.7133758068084717, 0.7961783409118652, 0.8566879034042358, 0.843949019908905, 0.7929936051368713, 0.8471337556838989, 0.8598726391792297, 0.8821656107902527, 0.837579607963562, 0.8471337556838989, 0.808917224407196, 0.8598726391792297, 0.8630573153495789, 0.8662420511245728, 0.8630573153495789, 0.8630573153495789, 0.837579607963562, 0.7961783409118652, 0.8853503465652466, 0.7961783409118652, 0.8726114630699158, 0.8630573153495789, 0.8566879034042358, 0.8630573153495789, 0.8216560482978821, 0.8121019005775452, 0.837579607963562, 0.8025477528572083, 0.8152866363525391, 0.8471337556838989, 0.8566879034042358, 0.824840784072876, 0.8630573153495789, 0.8471337556838989, 0.8025477528572083, 0.7802547812461853, 0.8184713125228882, 0.8535031676292419, 0.831210196018219, 0.8598726391792297, 0.824840784072876, 0.8726114630699158, 0.8630573153495789, 0.8535031676292419, 0.7484076619148254, 0.8662420511245728, 0.8598726391792297, 0.8503184914588928, 0.8757961988449097, 0.8407643437385559, 0.8407643437385559, 0.8025477528572083, 0.8598726391792297, 0.8630573153495789, 0.7929936051368713, 0.8121019005775452, 0.8694267272949219, 0.8471337556838989, 0.8503184914588928, 0.8407643437385559, 0.8216560482978821, 0.8535031676292419, 0.8726114630699158, 0.8216560482978821, 0.8566879034042358, 0.8503184914588928, 0.8662420511245728, 0.8535031676292419, 0.8598726391792297, 0.8216560482978821, 0.8662420511245728, 0.8471337556838989, 0.8694267272949219, 0.8503184914588928, 0.8566879034042358, 0.8598726391792297, 0.8598726391792297, 0.8025477528572083, 0.8598726391792297, 0.8503184914588928, 0.8662420511245728, 0.8853503465652466, 0.8598726391792297, 0.8343949317932129, 0.824840784072876, 0.8726114630699158, 0.7547770738601685, 0.8726114630699158, 0.8343949317932129, 0.8598726391792297, 0.8630573153495789, 0.8535031676292419, 0.8535031676292419, 0.8057324886322021, 0.8216560482978821, 0.8757961988449097, 0.8566879034042358, 0.831210196018219, 0.8630573153495789, 0.8789808750152588, 0.8662420511245728, 0.8694267272949219, 0.808917224407196, 0.843949019908905, 0.8407643437385559, 0.8535031676292419, 0.824840784072876, 0.824840784072876, 0.8566879034042358, 0.8598726391792297, 0.8407643437385559, 0.8535031676292419, 0.8630573153495789, 0.8630573153495789, 0.8598726391792297, 0.8503184914588928, 0.8407643437385559, 0.8757961988449097, 0.8184713125228882, 0.8471337556838989, 0.8216560482978821, 0.8566879034042358, 0.8535031676292419, 0.843949019908905, 0.831210196018219, 0.8598726391792297, 0.8566879034042358, 0.8726114630699158, 0.7770700454711914, 0.808917224407196, 0.8503184914588928, 0.8503184914588928, 0.8662420511245728, 0.8726114630699158, 0.8503184914588928, 0.8280254602432251, 0.8535031676292419, 0.8503184914588928, 0.8280254602432251, 0.8694267272949219, 0.8598726391792297, 0.8630573153495789, 0.8566879034042358, 0.837579607963562, 0.8535031676292419, 0.8535031676292419, 0.831210196018219, 0.8503184914588928, 0.8630573153495789, 0.8025477528572083, 0.8503184914588928, 0.8694267272949219, 0.8726114630699158, 0.8152866363525391, 0.8598726391792297, 0.8025477528572083, 0.8789808750152588, 0.8407643437385559, 0.8598726391792297, 0.8598726391792297, 0.7356687784194946, 0.8630573153495789, 0.8757961988449097, 0.8726114630699158, 0.8726114630699158, 0.8566879034042358, 0.8885350227355957, 0.8184713125228882, 0.8503184914588928, 0.824840784072876, 0.8630573153495789, 0.837579607963562, 0.8757961988449097, 0.8789808750152588, 0.8662420511245728, 0.8885350227355957, 0.8343949317932129, 0.824840784072876, 0.8280254602432251, 0.8025477528572083, 0.8057324886322021, 0.8566879034042358, 0.8535031676292419, 0.8949044346809387, 0.8216560482978821, 0.8662420511245728, 0.8885350227355957, 0.8694267272949219, 0.837579607963562, 0.8216560482978821, 0.8535031676292419, 0.843949019908905, 0.8566879034042358, 0.8566879034042358, 0.8471337556838989, 0.8789808750152588, 0.8503184914588928, 0.8280254602432251, 0.8535031676292419, 0.8757961988449097, 0.8726114630699158, 0.8535031676292419, 0.8566879034042358, 0.8662420511245728, 0.8630573153495789, 0.8566879034042358, 0.8726114630699158, 0.8630573153495789, 0.8471337556838989, 0.8694267272949219, 0.8630573153495789, 0.8535031676292419, 0.8025477528572083, 0.8566879034042358, 0.843949019908905, 0.8789808750152588, 0.8598726391792297, 0.8757961988449097, 0.8662420511245728, 0.8535031676292419, 0.8662420511245728, 0.8726114630699158, 0.8471337556838989, 0.8598726391792297, 0.8726114630699158, 0.8853503465652466, 0.8694267272949219, 0.8152866363525391, 0.8694267272949219, 0.831210196018219, 0.8152866363525391, 0.8566879034042358, 0.8917197585105896, 0.8853503465652466, 0.8694267272949219, 0.843949019908905, 0.8662420511245728, 0.8535031676292419, 0.8662420511245728, 0.8566879034042358, 0.8789808750152588, 0.837579607963562, 0.8566879034042358, 0.8821656107902527, 0.831210196018219, 0.8566879034042358, 0.8694267272949219, 0.8694267272949219, 0.8885350227355957, 0.8471337556838989, 0.843949019908905, 0.8407643437385559, 0.8184713125228882, 0.8184713125228882, 0.8184713125228882, 0.8694267272949219, 0.837579607963562, 0.837579607963562, 0.8343949317932129, 0.8853503465652466, 0.8407643437385559, 0.8757961988449097, 0.8630573153495789, 0.8757961988449097, 0.8885350227355957, 0.8757961988449097, 0.8821656107902527, 0.8598726391792297, 0.6878980994224548, 0.8694267272949219, 0.8598726391792297, 0.8885350227355957, 0.8917197585105896, 0.8949044346809387, 0.8630573153495789, 0.8630573153495789, 0.8407643437385559, 0.8662420511245728, 0.8821656107902527, 0.8057324886322021, 0.9012739062309265, 0.8726114630699158, 0.8630573153495789, 0.8662420511245728, 0.8566879034042358, 0.8503184914588928, 0.8694267272949219, 0.8471337556838989, 0.8566879034042358, 0.8757961988449097, 0.8726114630699158, 0.8757961988449097, 0.8471337556838989, 0.8694267272949219, 0.8980891704559326, 0.8917197585105896, 0.8726114630699158, 0.843949019908905, 0.8630573153495789, 0.843949019908905, 0.8535031676292419, 0.837579607963562, 0.8821656107902527, 0.7898089289665222, 0.8789808750152588, 0.7929936051368713, 0.8789808750152588, 0.8821656107902527, 0.8853503465652466, 0.8503184914588928, 0.8535031676292419, 0.8980891704559326, 0.8949044346809387, 0.8662420511245728, 0.8885350227355957, 0.7834395170211792, 0.8694267272949219, 0.8821656107902527, 0.8789808750152588, 0.8821656107902527, 0.8598726391792297, 0.8598726391792297, 0.9012739062309265, 0.8726114630699158, 0.8726114630699158, 0.8885350227355957, 0.8598726391792297, 0.8980891704559326, 0.8407643437385559, 0.8407643437385559, 0.8694267272949219, 0.808917224407196, 0.8630573153495789, 0.8789808750152588, 0.8535031676292419, 0.8853503465652466, 0.8662420511245728, 0.8885350227355957, 0.8853503465652466, 0.8471337556838989, 0.8757961988449097, 0.8853503465652466, 0.8917197585105896, 0.8280254602432251, 0.8757961988449097, 0.8694267272949219, 0.8280254602432251, 0.9076433181762695, 0.8853503465652466, 0.8980891704559326, 0.8917197585105896, 0.8917197585105896, 0.8694267272949219, 0.8917197585105896, 0.8917197585105896, 0.8566879034042358, 0.8694267272949219, 0.8503184914588928, 0.8503184914588928, 0.8694267272949219, 0.8662420511245728, 0.8853503465652466, 0.8789808750152588, 0.831210196018219, 0.8630573153495789, 0.8980891704559326, 0.9076433181762695, 0.8407643437385559, 0.8821656107902527, 0.8853503465652466, 0.8726114630699158, 0.8821656107902527, 0.8949044346809387, 0.8821656107902527, 0.8980891704559326, 0.8853503465652466, 0.8885350227355957, 0.843949019908905, 0.8694267272949219, 0.824840784072876, 0.8853503465652466, 0.8471337556838989, 0.9012739062309265, 0.8980891704559326, 0.8853503465652466, 0.8917197585105896, 0.8503184914588928, 0.8566879034042358, 0.8789808750152588, 0.8694267272949219, 0.8821656107902527, 0.8757961988449097, 0.8503184914588928, 0.8853503465652466, 0.8949044346809387, 0.8694267272949219, 0.8853503465652466, 0.8821656107902527, 0.8789808750152588, 0.843949019908905, 0.843949019908905, 0.8726114630699158, 0.8726114630699158, 0.8885350227355957, 0.8726114630699158, 0.8694267272949219, 0.8694267272949219, 0.8853503465652466, 0.8885350227355957, 0.8821656107902527, 0.8853503465652466, 0.8949044346809387, 0.843949019908905, 0.8917197585105896, 0.8885350227355957, 0.8917197585105896, 0.8980891704559326, 0.8917197585105896, 0.8821656107902527, 0.8216560482978821, 0.8598726391792297, 0.8949044346809387, 0.8917197585105896, 0.8757961988449097, 0.7898089289665222, 0.8566879034042358, 0.8980891704559326, 0.8757961988449097, 0.8821656107902527, 0.9012739062309265, 0.8630573153495789, 0.8949044346809387, 0.837579607963562, 0.8885350227355957, 0.8630573153495789, 0.8821656107902527, 0.8853503465652466, 0.8630573153495789, 0.8917197585105896, 0.8885350227355957, 0.8853503465652466, 0.8789808750152588, 0.8535031676292419, 0.8503184914588928, 0.8503184914588928, 0.8885350227355957, 0.8885350227355957, 0.8885350227355957, 0.8917197585105896, 0.8566879034042358, 0.8503184914588928, 0.8535031676292419, 0.8184713125228882, 0.8726114630699158, 0.8949044346809387, 0.8630573153495789, 0.8726114630699158, 0.8853503465652466, 0.9012739062309265, 0.8821656107902527, 0.8853503465652466, 0.8662420511245728, 0.8853503465652466, 0.831210196018219, 0.8757961988449097, 0.8821656107902527, 0.8630573153495789, 0.8949044346809387, 0.8694267272949219, 0.8535031676292419, 0.8630573153495789, 0.8917197585105896, 0.8757961988449097, 0.9012739062309265, 0.8821656107902527, 0.8789808750152588, 0.837579607963562, 0.843949019908905, 0.8726114630699158, 0.843949019908905, 0.8885350227355957, 0.8630573153495789, 0.8694267272949219, 0.8853503465652466, 0.8726114630699158, 0.8789808750152588, 0.824840784072876, 0.8853503465652466, 0.8917197585105896, 0.8821656107902527, 0.8885350227355957, 0.8789808750152588, 0.8789808750152588, 0.8407643437385559, 0.8535031676292419, 0.8535031676292419, 0.8598726391792297, 0.8917197585105896, 0.8980891704559326, 0.8821656107902527, 0.8853503465652466, 0.8694267272949219, 0.8694267272949219, 0.8917197585105896, 0.8853503465652466, 0.8566879034042358, 0.8853503465652466, 0.8980891704559326, 0.8757961988449097, 0.8598726391792297, 0.8853503465652466, 0.831210196018219, 0.9012739062309265, 0.8853503465652466, 0.8821656107902527, 0.8757961988449097, 0.8694267272949219, 0.8821656107902527, 0.8757961988449097, 0.8598726391792297, 0.9012739062309265, 0.8789808750152588, 0.8757961988449097, 0.8980891704559326, 0.8949044346809387, 0.824840784072876, 0.8598726391792297, 0.8853503465652466, 0.8885350227355957, 0.8853503465652466, 0.8789808750152588, 0.8503184914588928, 0.8821656107902527, 0.8280254602432251, 0.8821656107902527, 0.8980891704559326, 0.8598726391792297, 0.8980891704559326, 0.8885350227355957, 0.9044585824012756, 0.9012739062309265, 0.8535031676292419, 0.8471337556838989, 0.8853503465652466, 0.8853503465652466, 0.8726114630699158, 0.8980891704559326, 0.8789808750152588, 0.8471337556838989, 0.8598726391792297, 0.8917197585105896, 0.9012739062309265, 0.8726114630699158, 0.8726114630699158, 0.8853503465652466, 0.8662420511245728, 0.8917197585105896, 0.8630573153495789, 0.8757961988449097, 0.8949044346809387, 0.8853503465652466, 0.8949044346809387, 0.8184713125228882, 0.8566879034042358, 0.8885350227355957, 0.8821656107902527, 0.8662420511245728, 0.8662420511245728, 0.8789808750152588, 0.8949044346809387, 0.9012739062309265, 0.8789808750152588, 0.8662420511245728, 0.8694267272949219, 0.8980891704559326, 0.9044585824012756, 0.8662420511245728, 0.8630573153495789, 0.8821656107902527, 0.8853503465652466, 0.8662420511245728, 0.8694267272949219, 0.9012739062309265, 0.8757961988449097, 0.8949044346809387, 0.8980891704559326, 0.8757961988449097, 0.8630573153495789, 0.8726114630699158, 0.9044585824012756, 0.8757961988449097, 0.8853503465652466, 0.8853503465652466, 0.8885350227355957, 0.8789808750152588, 0.8630573153495789, 0.8757961988449097, 0.831210196018219, 0.8789808750152588, 0.8821656107902527, 0.8726114630699158, 0.8694267272949219, 0.8726114630699158, 0.8757961988449097, 0.9044585824012756, 0.8949044346809387, 0.8885350227355957, 0.8757961988449097, 0.8757961988449097, 0.8726114630699158, 0.8789808750152588, 0.808917224407196, 0.837579607963562, 0.8853503465652466, 0.9044585824012756, 0.8885350227355957, 0.8853503465652466, 0.8757961988449097, 0.8662420511245728, 0.8789808750152588, 0.8821656107902527, 0.8726114630699158, 0.8853503465652466, 0.8757961988449097, 0.8757961988449097, 0.8662420511245728, 0.8694267272949219, 0.8726114630699158, 0.8853503465652466, 0.8917197585105896, 0.8789808750152588, 0.8630573153495789, 0.8757961988449097, 0.9076433181762695, 0.8917197585105896, 0.8853503465652466, 0.8853503465652466, 0.8949044346809387, 0.843949019908905, 0.8662420511245728, 0.8917197585105896, 0.8726114630699158, 0.8694267272949219, 0.8980891704559326, 0.8821656107902527, 0.8885350227355957, 0.8885350227355957, 0.8980891704559326, 0.9044585824012756, 0.8630573153495789, 0.8853503465652466, 0.8885350227355957, 0.8789808750152588, 0.8789808750152588, 0.7675158977508545, 0.7993630766868591, 0.837579607963562, 0.8853503465652466, 0.9044585824012756, 0.8789808750152588, 0.9076433181762695, 0.8853503465652466, 0.9012739062309265, 0.8917197585105896, 0.8917197585105896, 0.8821656107902527, 0.9140127301216125, 0.8949044346809387, 0.8980891704559326, 0.8949044346809387, 0.8949044346809387, 0.8917197585105896, 0.8980891704559326, 0.8853503465652466, 0.9076433181762695, 0.8885350227355957, 0.8980891704559326, 0.9076433181762695, 0.8980891704559326, 0.8949044346809387, 0.8980891704559326, 0.8980891704559326, 0.8885350227355957, 0.8917197585105896, 0.8949044346809387, 0.8821656107902527, 0.8821656107902527, 0.8503184914588928, 0.8821656107902527, 0.8757961988449097, 0.8853503465652466, 0.8789808750152588, 0.8980891704559326, 0.8726114630699158, 0.9044585824012756, 0.8980891704559326, 0.8980891704559326, 0.9044585824012756, 0.9140127301216125, 0.8949044346809387, 0.8949044346809387, 0.8885350227355957, 0.8662420511245728, 0.8917197585105896, 0.8885350227355957, 0.8949044346809387, 0.9044585824012756, 0.8885350227355957, 0.8980891704559326, 0.8789808750152588, 0.8789808750152588, 0.8853503465652466, 0.8917197585105896, 0.8789808750152588, 0.8949044346809387, 0.8917197585105896, 0.8949044346809387, 0.8598726391792297, 0.9044585824012756, 0.9076433181762695, 0.9012739062309265, 0.8949044346809387, 0.8980891704559326, 0.9012739062309265, 0.7929936051368713, 0.8853503465652466, 0.8216560482978821, 0.8535031676292419, 0.8789808750152588, 0.8885350227355957, 0.8694267272949219, 0.8821656107902527, 0.9012739062309265, 0.8821656107902527, 0.8885350227355957, 0.8917197585105896, 0.8917197585105896, 0.8980891704559326, 0.9012739062309265, 0.8885350227355957, 0.8789808750152588, 0.9044585824012756, 0.8726114630699158, 0.8853503465652466, 0.8917197585105896, 0.9044585824012756, 0.8980891704559326, 0.8821656107902527, 0.8980891704559326, 0.8789808750152588, 0.9044585824012756, 0.8980891704559326, 0.8949044346809387, 0.8949044346809387, 0.8789808750152588, 0.9108280539512634, 0.9012739062309265, 0.8789808750152588, 0.8885350227355957, 0.8885350227355957, 0.8757961988449097, 0.8980891704559326, 0.9076433181762695, 0.9012739062309265, 0.8757961988449097, 0.8980891704559326, 0.8949044346809387, 0.8885350227355957, 0.8757961988449097, 0.8885350227355957, 0.8949044346809387, 0.8949044346809387, 0.8980891704559326, 0.8949044346809387, 0.8980891704559326, 0.8853503465652466, 0.8949044346809387, 0.8853503465652466, 0.8789808750152588, 0.8980891704559326, 0.8885350227355957, 0.8789808750152588, 0.9076433181762695, 0.9044585824012756, 0.8853503465652466, 0.8821656107902527, 0.9076433181762695, 0.8726114630699158, 0.8949044346809387, 0.8885350227355957, 0.9012739062309265, 0.8694267272949219, 0.8726114630699158, 0.8980891704559326, 0.9044585824012756, 0.9076433181762695, 0.9044585824012756, 0.8917197585105896, 0.8789808750152588, 0.8694267272949219, 0.8853503465652466, 0.8662420511245728, 0.8694267272949219, 0.8662420511245728, 0.8917197585105896, 0.8885350227355957, 0.8917197585105896, 0.8917197585105896, 0.8853503465652466, 0.9044585824012756, 0.8726114630699158, 0.8980891704559326, 0.8980891704559326, 0.8853503465652466, 0.9108280539512634, 0.9140127301216125, 0.9044585824012756, 0.9044585824012756, 0.9012739062309265, 0.8949044346809387, 0.8726114630699158, 0.8757961988449097, 0.9171974658966064, 0.8885350227355957, 0.8885350227355957, 0.8949044346809387, 0.8917197585105896, 0.8885350227355957, 0.8980891704559326, 0.8885350227355957, 0.9044585824012756, 0.9012739062309265, 0.9044585824012756, 0.8980891704559326, 0.8853503465652466, 0.9044585824012756, 0.8821656107902527, 0.9012739062309265, 0.9076433181762695, 0.8949044346809387, 0.9108280539512634, 0.8726114630699158, 0.8598726391792297, 0.8917197585105896, 0.8917197585105896, 0.9044585824012756, 0.8726114630699158, 0.8789808750152588, 0.8757961988449097, 0.8726114630699158, 0.8917197585105896, 0.8853503465652466, 0.8917197585105896, 0.8789808750152588, 0.8917197585105896, 0.8885350227355957, 0.9203821420669556, 0.8853503465652466, 0.9108280539512634, 0.8789808750152588, 0.9108280539512634, 0.8726114630699158, 0.8917197585105896, 0.9044585824012756, 0.8821656107902527, 0.8949044346809387, 0.8980891704559326]
# plt.plot(epochs, acc, 'bo', label='Training acc')  # 'bo'为画蓝色圆点，不连线
# plt.plot(epochs, val_acc, 'b', label='Validation acc')
# plt.title('Training and validation accuracy')
# plt.legend()  # 绘制图例，默认在右上角
# plt.savefig("Training and validation accuracy.png",dpi=300)



















